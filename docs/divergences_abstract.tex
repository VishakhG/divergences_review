Generative modeling can be seen as the task of learning a model from data that we can sample new points from. This often translates into learning a probability distribution that is close to some real data distribution from which we have access to samples. In recent years deep neural networks have been leveraged to amortize inference across observed data in latent variable models, sample new points via powerful implicit models and more. 

To phrase the more general task of unsupervised learning formally: view the data as a sample from an underlying probability distribution $\mathcal{Q}$ defined over a Polish space $\mathcal{X}$ (Complete and separable whose topology comes from a distance function). Denote by $\mathcal{P}_{\mathcal{X}}$ the space of probability measures $\mu$ defined on $(\mathcal{X},\mathcal{U})$, $\mathcal{U}:\text{\textit{Borel} $\sigma$-algebra generated by open sets of $\mathcal{X}$}$. Consider a way to compare elements of $\mathcal{P}_{\mathcal{X}}: (P,Q) \rightarrow D(P,Q) \in [0, \infty)$, The goal, given a family of distributions $\mathcal{P}_\theta \in P_X$, is to find $\min_{\theta}L(\theta)=D(Q, P_{\theta})$ ~\cite{1712.07822}. One interesting question is how should we choose such a  measure of closeness $D$ between the data distribution $Q$ and the learned distribution $P_\theta$? Different choices of $D$ lead to different modeling decisions and properties in our final learned model.

In this project we will explore the different classes of divergences and distances between probability distributions and try to relate desiderata for divergences/distances with the classes of such measures that are consistent with those desiderata. Additionally, we will try to develop a set of ideal desiderata that a divergence/distance should  obey for generative modeling and demonstrate whether or not such a measure exists.

In order to examine these questions we will first review existing classes of divergences and distances between distributions and try to place them in a taxonomy showing how they are compare and contrast. Then we will review different popular generative models and elaborate on the connections between them and which underlying $D$ they minimize. In cases where the underlying divergence/distance is unclear we will also try and investigate what it actually is. Finally we will develop an ideal set of desiderata for a divergence/distance in the context of generative modeling and show whether any measure is consistent with them.

Generative models are a very popular approach to unsupervised learning, but without understanding the connections between different modeling approaches and the underlying divergences/distances they minimize, it is difficult to improve on them. Furthermore, If we have a clear understanding of how our choice of measure $D$ will be reflected in our modeling decisions and final learned model, then we can make informed decisions on a case by case basis.