<!doctype html>
<meta charset="utf-8">
<script src="https://distill.pub/template.v1.js"></script>

<script type="text/front-matter">
  title: "Divergences between distributions"
  description: "Review of choices made when generative modeling"
  authors:
  - Vishakh Gopu: "vishakh.me"
  affiliations:
  - NYU: vishakh.me
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
  tex2jax: {
  inlineMath: [ ['$','$'], ["\\(","\\)"] ],
  processEscapes: true
  }
  });
</script>

<script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<dt-article>
<h2> Motivation </h2>
    <p>
        Generative modeling is the task of learning a model from data that we
        can sample new points from. This often translates into learning a
        probability distribution that is close to some real data distribution
        from which we have access to samples. In recent years deep neural
        networks have been leveraged to amortize inference across observed data
        in latent variable models, sample new points via flexible implicit
        models and more.
    </p>

<p>
    In recent years a plethora of generative models have been introduced, often
    accompanied by a catchy name and impressive claims. The object of this review
    is to show the landscape of generative models in a taxonomy showing their
    connections and differences. A specific generative modeling approach can then be
    seen as the consequence of a series of different modeling choices rather than an
    ad hoc invention. From this vantage point we can tailor our generative modeling
    approach to the problem at hand as a practitioner and wisely choose where we
    wish to explore next as a researcher.

    Before diving into the various choices that can be made during the generative
    modeling process, let us give a definition for the task at hand.
</p>

<h2>Introduction</h2>

<p>
    Generative modeling is the task of learning a model from data that we can
    sample new points from. This often translates into learning a probability
    distribution that is close to some real data distribution from which we have
    access to samples. Additionally we often wish to learn interesting structure
    from data, or impose structure we know exists into the generative model.
    In recent years deep neural networks have been leveraged to amortize
    inference across observed data in latent variable models,
    sample new points via flexible implicit models and more, we will refer to
    this as <i>Deep</i> generative modeling.
</p>

<p><b>Phrased Formally:</b></p>

<p>
    View the data as a sample from an underlying probability distribution
    $\mathcal{Q}$ defined over a Polish space $\mathcal{X}$
    (Complete and separable whose topology comes from a distance function).

    Denote by $\mathcal{P}_{\mathcal{X}}$ the space of probability measures
    $\mu$ defined on $(\mathcal{X}, \mathcal{U})$,
    $\mathcal{U}$: Borel $\sigma$-algebra generated by open sets of $\mathcal{X}$.
    Consider a way to compare elements of $\mathcal{P}_{\mathcal{X}}: (P,Q)
    \rightarrow D(P,Q) \in [0, \infty)$,

    The goal, given a family of distributions $\mathcal{P}_\theta \in P_X$,
    is to find $\min_{\theta}L(\theta)=D(Q, P_{\theta})$
    <dt-cite key="bottougeo"></dt-cite>.
</p>

<p>
From this definition we can start to dilineate some of the high
level choices that end up defining a generative modeling procedure.
</p>

<ul>
    <li>
        <b>Decision one:</b>
        Choice of $D$
    <li>
        <b>Decision two:</b>
        Principle of learning: how to $\min_{\theta}L(\theta)=D(Q, P_{\theta})$
    </li>

    <li><b>Decision three:</b>
        How to model $\mathcal{P}_\theta$</li>
    </li>
</ul>

<p>
    Many popular generative models in use today can be indexed by their answers
    to the above questions. There are obviously many finer points of distinction
    but that is beyond the scope of this survey. We will tackle each in turn,
    paying special attention to the choice of divergence. We will then discuss
    popular models and how they fit in in the above taxonomy.
    Finally we will discuss desiderata for the choice of divergence $D$,
    examine how the choice of $D$ impacts the final
    the final learned model and how that relates to downstream tasks.
    Finally we will attempt to see how which choices of $D$ are
    compatible with which sets of desiderata for a downstream task.
</p>

<h2>Choice of $D$</h2>
<p>
    One of the most important modeling decisions is the choice of divergence
    or distance between $P_R$ and $P_G$.

    To qualify as a true distance a measure $\textit{d}$ must fulfill:
    <ol>
        <li>$d(x, x)=0$
            This is a common sense property for any distance to have.
        </li>

        <li>$x \neq y \implies d(x,y)>0$
            This is called <i>seperation</i> and is also a common sense requirement
        that follows from (1).
        </li>

        <li>$d(x,y) = d(y,x)$
            This symetry may be nice to have.
        </li>

        <li>$d(x, y) \leq d(x, z) + d(z, y)$
            This is the triangle inequality.
        </li>
    </ol>
<p>
<p>
    When a choice $d$ fails to fulfill (3) or (4), we will call it a divergence.
    To encompass both divergences and distances we will refer to $D$ from our
    original definition of generative modeling.
</p>

<p>
    Now lets take a look at the different families from which to draw $D$,
    enumerate their members, and discuss their characteristics.
</p>

<h3>F-divergences</h3>

    <p><u>Description</u></p>
    <p>
        Let $P$ and $Q$ be two probability distributions over a space
        $\Omega$ such that $P$ is absolutely continuous with respect to $Q$.
        Then, for a convex function $f$ such that $f(1)=0$ the
        <i>f-divergence</i> of $P$ from $Q$ is defined as:

        $$D_f(P || Q) = \int_{\Omega} f(\frac{dP}{dQ})dQ$$.
    </p>

    <p><u> Members </u></p>
    <p>
        <table>
        <tbody>
        <tr style="height: 23px;">
        <td style="height: 23px;"><b>Name<b></td>
        <td style="height: 23px;">$\mathbf{\mathcal{f}(t)}$</td>
        </tr>
        <tr style="height: 23px;">
        <td style="height: 23px;">KL-divergence</td>
        <td style="height: 23px;">$t \log t$</td>
        </tr>
        <tr style="height: 23px;">
        <td style="height: 23px;">Reverse KL-divergence</td>
        <td style="height: 23px;">$-\log t$</td>
        </tr>
        <tr style="height: 23px;">
        <td style="height: 23px;">Alpha divergence</td>
        <td style="height: 23px;"><br />$ <br /> \begin{cases} <br />
            \alpha \neq \pm 1 &amp; \frac{4}{1-\alpha^2}(1-t^{\frac{(1+ \alpha)}{2}}) \\
            <br /> \alpha = 1 &amp; t \ln t \\<br /> \alpha = -1 &amp; -\ln t<br />
            \end{cases}<br /><br /><br />$
        </td>
        </tr>
        <tr style="height: 23px;">
        <td style="height: 23px;">Squared hellinger</td>
        <td style="height: 23px;">$(\sqrt{t} -1)^2$</td>
        </tr>
        <tr style="height: 23px;">
        <td style="height: 23px;">Jensen shannon</td>
        <td style="height: 23px;">$-(t+1)\log \frac{1+u}{2} +u \log u$</td>
        </tr>
        <tr style="height: 23px;">
        <td style="height: 23px;">Pearson $\mathcal{X}^2$</td>
        <td style="height: 23px;">$(u-1)^2$</td>
        </tr>
        <tr style="height: 23px;">
        <td style="height: 23px;">Total variation</td>
        <td style="height: 23px;">$\frac{1}{2} |t-1|$</td>
        </tr>
        </tbody>
        </table>
    </p>

    <p><u>Analysis</u></p>
    <p>TODO</p>

<h3>Integral Probability Metrics</h3>
    <p><u>Description</u></p>
    <p>
        Given a real valued, bounded and measureable set of functions
        $\mathcal{F}$ on $M$:
        $$
            \gamma_{\mathcal{F}}(P, Q) =
            \sup_{f \in \mathcal{F}}|\int_{M} f dP - \int_M f dQ|
        $$
    </p>

    <p><u>Members</u></p>
    <p>Given a metric space $(M, \rho)$ with $\mathcal{A}$ beging a borel
    sigma algebra induced by metric topology. let $mathcal{P}$ be the set
of all Borel probability measures on $\mathcal{A}$</p>
    <p>
        <table style="width: 675px;">
        <tbody>
        <tr style="height: 23px;">
        <td style="height: 23px; width: 188px;"><strong>Name</strong>&nbsp;</td>
        <td style="height: 23px; width: 486px;"><strong>Choice of $\mathcal{F}$</strong>&nbsp;</td>
        </tr>
        <tr style="height: 23px;">
        <td style="height: 23px; width: 188px;">Dudley metric</td>
        <td style="height: 23px; width: 486px;">
        <p>&nbsp;$\mathcal{F} = \{f : ||f||_{BL} \leq 1\}$</p>
        <p>&nbsp;Where:</p>
        <p>&nbsp;$||f||_{BL} := ||f||_{\infty} + ||f||_L$</p>
        <p>&nbsp;$||f||_{\infty}:= \sup\{|f(x)| : x \in M\}$</p>
        <p>&nbsp;$||f||_L:=\sup\{|f(x) -f(y)|\rho(x,y) :x \neq y \in M\}$</p>
        </td>
        </tr>
        <tr style="height: 23px;">
        <td style="height: 23px; width: 188px;">Maximum mean discrepancy</td>
        <td style="height: 23px; width: 486px;">
        <p>&nbsp;$\mathcal{F} = \{f: ||f||_{\mathcal{H}} \leq 1 \}$</p>
        <p>&nbsp;$H$ is a reproducing kernel hilbert space with $k$ as its reproducing kernel.</p>
        </td>
        </tr>
        <tr style="height: 23px;">
        <td style="height: 23px; width: 188px;">Kantorovich metric</td>
        <td style="height: 23px; width: 486px;">&nbsp;$\mathcal{F}=\{f: ||f||_L \leq 1 \}$</td>
        </tr>
        <tr style="height: 23px;">
        <td style="height: 23px; width: 188px;">Total variation distance</td>
        <td style="height: 23px; width: 486px;">&nbsp;$\mathcal{F} =\{f: ||f||_\infty \leq 1\}$&nbsp;</td>
        </tr>
        <tr style="height: 23px;">
        <td style="height: 23px; width: 188px;">Kolmogorov distance</td>
        <td style="height: 23px; width: 486px;">&nbsp;$\mathcal{F}=\{\mathcal{1}_{(\infty, t]}, t \in \mathbb{R}^d\}$</td>
        </tr>
        </tbody>
        </table>
    </p>

    <p><u>Analysis</u></p>
    <p>TODO</p>

<h4>Bregman divergences</h4>
<p><u>Description</u></p>
<p>
    For two continuous distributions $\mathbb{P}$, $\mathbb{Q}$ and a strictly
    convex function $\phi(x):\mathbb{R} \to \mathbb{R}$
        $$D_{\phi}(\mathbb{P}, \mathbb{Q}) = \int[
            \phi(p(x))-\phi(q(x)) - \phi'(q(x))(p(x) - q(x))]d\mu(x)
        ]$$
        where $p(x)$ and $q(x)$ are the probability density functions of
        $\mathbb{P}$ and $\mathbb{Q}$ respectively and $\mu$ is the base measure.
</p>
<p><u>Members</u></p>
<p>
        <table>
    <tbody>
    <tr>
    <td>&nbsp;<strong>Name</strong></td>
    <td><strong>Choice of</strong> $\mathbb{\phi}$</td>
    </tr>
    <tr>
    <td>&nbsp;$L^2$ loss</td>
    <td>&nbsp; $\phi(x)= ||x||_{2}^2$</td>
    </tr>
    <tr>
    <td>&nbsp;Itakura-Saito divergence</td>
    <td>&nbsp; $\phi(x) = -log(x)$</td>
    </tr>
    <tr>
    <td>&nbsp;KL divergence</td>
    <td>&nbsp; $\phi(x) = \sum_{i=1}^d x_i log x_i$</td>
    </tr>
    <tr>
    <td>&nbsp;Mahalonobis distance</td>
    <td>&nbsp; $\phi(x) = x^T A x$ where A is positive definite</td>
    </tr>
    </tbody>
    </table>
</p>
<p><u>Analysis</u></p>
    <p>TODO</p>
<h3>Decision 2: choice of model/representation</h3>
<p>
There are many different ways to model $P_{\theta}$, in this review
we will focus on a few general trends.
</p>

<p><u>Latent variable models</u></p>
<p><u>Implicit models</u></p>

<h3>Decision 3: Principle of learning</h3>
<p>
    An important criteria that can defines a generative modeling procedure is
    the principle of learning. There are many different specific mechanisms for learning in generative models including:
    <ul>
        <li>Exact methods</li>
        <li>Nmerical integration</li>
        <li>Generalized method of moments</li>
        <li>Maximum A posteriori</li>
        <li>Laplace approximation</li>
        <li>Expectation maximization</li>
        <li>Monte carlo</li>
        <li>Variational methods</li>
    </ul>
</p>

<p>
Since we are considering the broader goal of minimizing $D(P_{\theta}, P_{R})$
in the context of deep generative models lets consider two approaches for doing
this:
</p>

<ol>
    <li>Maximum likelihood estimation</li>

    <li>Estimation by comparison.</li>

</ol>

<p><u>Maximum likelihood</u></p>
<p>
    One popular approach to finding an optimal $P_{\theta}$ is to set theta
    so that the data we observe is maximally probable:
    $\max_{\theta \in \mathbb{R}\sum_{i}^{m} log P_{\theta}(x_i)}$
    This is equivalent to minimizing $KL(P_{r}, P_{\theta})$ asymptotically.
</p>

<p><u> Learning by comparison</u></p>
<p>
    For the following discussion lets refer to the real distribution
    as $p^*$ and our approximating distribution as $q$.

    Often we don't have access to the likelihood function $P(D|\theta|)$ but
    can easily sample from our model $q_{\phi}$
    and have access to samples from the data generating distribution $p^{*}$.
    Also lets say our model maps a latent vector $z$ to a sample.
    In this case our only hope is to proceed with learning by comparing
    samples from $q_{\phi}$ and $p^*$ and adjust $\theta$ to make this
    comparison more favorable. Consider the ratio
    $r_{\phi} = \frac{q}{p^*}$. When $r_{\phi}$ is 1, the distributions are
    identical. We can try and develop a learning procedure that attempts to
    improve $r_{\phi}$ by alternating between <i>Comparison</i> and
    <i>Estimation</i> steps.

    In the <i> Comparison step</i> we build an auxilliary model to evaluate
    differences between observed data and simulated data.
    In the <i>estimation</i> step we adjust model
    parameters. The general framework for this type of learning is as follows:
    First combine the real data and simulated data to form
    $\{\boldsymbol{x_1}..\boldsymbol{x_N}\}$.
    Next assign them binary labels (+1, -1) respectively, denoting their
    class membership.
    Observe that $p^*(\boldsymbol{x})=p(\boldsymbol{x}|y=1)$ and
    $q(\boldsymbol{x}) = p(\boldsymbol{x}|y=-1)$. W can now write the ratio
    as $\frac{p^*(\boldsymbol{x})}{q(\boldsymbol{x})} =
    \frac{p(\boldsymbol{x}|y=1)}{p(\boldsymbol{x}|y=-1)}$

    Through a bayes
    substitution this equals
    $\frac{p(y=+1|\boldsymbol{x})p(x)}{p(y=+1)} / \frac{p(y=-1|\boldsymbol{x})p(x)}{p(y=-1)}$
    This allows us to equate $r_{\phi}$ with the class probability ratio
    $\frac{p(y=1|x)}{p(y=-1|x)}$.

     Next assume a scoring function $D_{\theta}(\boldsymbol{x}) = p(y=+1|\boldsymbol{x})$.

    Since we have binary labels it makes sense to use a  bernoulli loss:
    $\mathcal{F}(\boldsymbol{x}, \theta, \phi) = E_{p^*(x)}[\log D_{\theta}(\boldsymbol{x})]
    + E_{q_{\phi}}(x)[\log (1-D_{\theta}(\boldsymbol{x}))]]$

    Our optimization can now be
    $min_{\phi}max_{\theta}\mathcal{F}(\boldsymbol{x}, \theta, \phi)$
    This gives the two alternating steps with respect to each set of parameters:
    <ol>
    <li>
        Update $\theta$: $\nabla_{\theta} E_{p^*}(x)[\log D_{\theta}(\boldsymbol{x})]
         + \nabla_{\theta} E_{q_{\phi}}(x)[log(1-D_{\theta}(\boldsymbol{x}))]$
    </li>
    <li>
        Update $\phi$: $-\nabla_{\phi}E_{q(z)}
        [log D_{\theta}(f_{\phi}(\boldsymbol{z}))]$
    </li>
    </ol>
</p>

<h3>Popular models</h3>

<h3>A closer look at $D$</h3>
<p><u>Intuition</u></p>
<p>
    The choice of $D$ has a big impact on the final learned model. To gain some
    intuition, we show here images of an isotropic guassian trying to fit data
    from a random mixture of gaussians under various choices of $D$. Though
    things are very different in high dimensions, we can start seeing the kinds
    of trade offs being made by visualizing this simple setting.
</p>
<p>
    <div class = "row">
    <figure>
      <img src="figures/wasserstein_gan_samples.png" height="200" width="200"/>
      <figcaption>Wasserstein distance</figcaption>
    </figure>
    <figure>
        <img src="figures/F-gan samples with divergence forwardkl.png" height="200" width="200"/>
        <figcaption>Forward KL</figcaption>
      </figure>
      <figure>
        <img src="figures/F-gan samples with divergence pearsonchisquared.png" height="200" width="200"/>
        <figcaption>Pearson Chi-squared</figcaption>
      </figure>
      <figure>
        <img src="figures/F-gan samples with divergence reversekl.png" height="200" width="200"/>
        <figcaption>Reverse KL</figcaption>
      </figure>
    </div>
    <div class="row">
      <figure>
        <img src="figures/F-gan samples with divergence squaredhellinger.png" height="200" width="200"/>
        <figcaption>Squared Hellinger</figcaption>
      </figure>

      <figure>
        <img src="figures/F-gan samples with divergence totalvariation.png" height="200" width="200"/>
        <figcaption>Total Variation</figcaption>
      </figure>
      <figure>
        <img src="figures/eb_gan_samples.png" height="200" width="200"/>
        <figcaption>Energy based gans</figcaption>
      </figure>
    </div>
</p>

<h3>Downstream tasks and desiderata</h3>

</dt-article>
